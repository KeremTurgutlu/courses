{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information, Self-information and Split Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we want to see how split-learning is helping privacy by decreasing the information content in a raw input. This is a simplified example, just to help the understanding of the concepts in this lesson. \n",
    "\n",
    "We will be using MNIST data for this exercise. We will first download and load the data. Then, we will load a pretrained small DNN. Our aim is to compare the information in the raw inputs of the MNIST test set, with the information in the output of the final convolution layer, to see if there is information degredation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from lenet_5 import LeNet5_5\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BATCH_TEST_SIZE = 1024\n",
    "data_train = MNIST('/Users/turgutlu/.syft/data/',\n",
    "                   download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor()]))\n",
    "data_test = MNIST('/Users/turgutlu/.syft/data/',\n",
    "                  train=False,\n",
    "                  download=False,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))\n",
    "data_train_loader = DataLoader(data_train, batch_size = BATCH_SIZE , shuffle=True, num_workers=8)\n",
    "data_test_loader = DataLoader(data_test,  batch_size = BATCH_TEST_SIZE, num_workers=8)\n",
    "data_test_loader2 = DataLoader(data_test,  batch_size = 1, num_workers=0)\n",
    "\n",
    "TRAIN_SIZE = len(data_train_loader.dataset)\n",
    "TEST_SIZE = len(data_test_loader.dataset)\n",
    "NUM_BATCHES = len(data_train_loader)\n",
    "NUM_TEST_BATCHES = len(data_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = LeNet5_5()\n",
    "model_loaded.load_state_dict(torch.load(\"./LeNet-saved-5\"))\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate (net, criterion):\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_test_loader):\n",
    "        labels = (labels > 5).long()\n",
    "        output = net(images)\n",
    "        avg_loss += criterion(output, labels).sum()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "\n",
    "    avg_loss /= len(data_test)\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run validate to check the accuracy of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Avg. Loss: 0.000023, Accuracy: 0.992900\n"
     ]
    }
   ],
   "source": [
    "validate (model_loaded, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5_5(\n",
       "  (convnet): Sequential(\n",
       "    (c1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (s2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (c3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (relu3): ReLU()\n",
       "    (s4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (c5): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (relu5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (f6): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (relu6): ReLU()\n",
       "    (f7): Linear(in_features=84, out_features=2, bias=True)\n",
       "    (sig7): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and measuring information content\n",
    "\n",
    "At this point, we want to split the network to two parts, and observe how different the information content of the original images and the intermediate activations are. We chose the last convolution layer of the pre-trained model we had as the splitting point. We will feed all the test data to the convolutions, and save their outputs so that we can later use them to quantitatively measure the bits of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the raw images and the intermediate activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b025ea274146e7963ba0e8fad2ccc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "intermediate_activations = []\n",
    "total_correct = 0\n",
    "\n",
    "model_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in tqdm(enumerate(data_test_loader2), total=len(data_test_loader2)):\n",
    "        imgs.append(images.squeeze(0).view(1,-1))\n",
    "\n",
    "        x = model_loaded.convnet(images)\n",
    "        intermediate_activations.append(x.view(1,120))\n",
    "\n",
    "    np.save(\"images\", torch.cat(imgs).numpy())\n",
    "    np.save(\"intermediate_act\", torch.cat(intermediate_activations).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Information Toolbox\n",
    "\n",
    "We'll be using the Information toolbox to calculate mutual information. Let's load it here. If the `ite-repo` folder isn't in the same directory where you're running this notebook, change the file path to the correct location below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'./ite-repo')\n",
    "import ite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll load the raw images and intermediate activations as Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1024)\n",
      "(10000, 120)\n"
     ]
    }
   ],
   "source": [
    "images_raw=np.load(\"images.npy\")\n",
    "print(images_raw.shape)\n",
    "intermediate_activation=np.load(\"intermediate_act.npy\")\n",
    "print(intermediate_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate self-information and mutual information of the first 1,000 test images. You can remove the slicing to calculate on full 10,000 test images but it will take longer to run.\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\sum\\limits_{(x,y) \\in \\mathcal{X}\\times\\mathcal{Y}} P_{XY}(x,y) \\log \\frac{P_{XY}(x,y)}{P_X(x)P_Y(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = ite.cost.MIShannon_DKL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll calculate the self-information of the raw images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2048)\n",
      "455.97089317698345\n"
     ]
    }
   ],
   "source": [
    "ds = np.array([1024, 1024])\n",
    "y = np.concatenate((images_raw[:1000], images_raw[:1000]),axis=1)\n",
    "print(y.shape)\n",
    "i = co.estimation(y, ds) \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll calculate the mutual information between the raw images and the intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1144)\n",
      "163.148586532957\n"
     ]
    }
   ],
   "source": [
    "ds = np.array([1024, 120])\n",
    "y = np.concatenate((images_raw[:1000], intermediate_activation[:1000]),axis=1)\n",
    "print(y.shape)\n",
    "i = co.estimation(y, ds) \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the raw image contained 455 bits of self-information, whereas the intermediate activations only contain 163 bits of information that was originally in the raw image (the 455 bits). This shows that the first layers of the neural network, alone, have degraded more than half of the original information in the raw input. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
